# Sentiment Analysis for Product Reviews (Amazon)

**Industry:** E-commerce  
**Goal:** Analyze customer reviews to determine sentiment (**positive / neutral / negative**) and inform product improvements.

---

## Dataset
- **Source:** Amazon Product Reviews (Kaggle) — add link: `https://www.kaggle.com/datasets/datafiniti/consumer-reviews-of-amazon-products?resource=download`
- Raw schema (subset used): `reviews.id, reviews.date, asins, brand, reviews.rating, reviews.title, reviews.text, ...`

---

## Approach

1. **Load & Preprocess**
   - Tokenize → remove punctuation & stopwords → lowercase → lemmatize
   - Output: `data/processed/reviews_clean.csv` with columns:
     - `text` *(title + ". " + review body — used for matching back to raw)*  
     - `label` *(gold sentiment for training)*  
     - `text_clean` *(cleaned string used by models)*

2. **Feature Extraction**
   - **TF-IDF** (1–2 n-grams, `max_features=50k`, `min_df=2`) for classical models
   - (Deep model tried) **embedding + BiLSTM** for comparison

3. **Models Trained**
   - Logistic Regression (baseline)
   - Linear SVM
   - Linear SVM + **RandomOverSampler**
   - Linear SVM **Hybrid** (undersample majority + oversample minority) ✅
   - BiLSTM (embeddings)

4. **Evaluation**
   - Metrics: **Accuracy**, **Macro Precision/Recall/F1**
   - Confusion matrix on untouched test split
   - Results summary (`outputs/comparison.csv`):

     | Model            | Accuracy | Macro_Precision | Macro_Recall | Macro_F1 | Notes                          |
     |------------------|---------:|----------------:|-------------:|---------:|--------------------------------|
     | Logistic Regression | 0.672 | 0.65 | 0.51 | 0.36 | Baseline TF-IDF + LogReg |
     | SVM              | 0.923 | 0.56 | 0.53 | 0.54 | Linear SVM |
     | SVM Oversampled  | 0.926 | 0.58 | 0.55 | 0.56 | RandomOverSampler |
     | **SVM Hybrid**   | **0.935** | **0.64** | **0.55** | **0.58** | **SMOTE + undersampling (chosen)** |
     | LSTM             | 0.926 | 0.60 | 0.48 | 0.52 | BiLSTM with embeddings |

Additional artifacts:
- Confusion matrix PNGs in `outputs/charts/` (generated by training scripts).

5. **Dashboard - Export for Analysis / BI**
   - Predictions exported to `outputs/predictions.csv` for BI visualization.

---

## Repository Structure

```text
.
├── src/
│   ├── train_baseline.py
│   ├── train_svm.py
│   ├── train_svm_undersample.py
│   ├── train_svm_resample.py
│   ├── train_svm_hybrid.py            # chosen model
│   ├── score_reviews.py               # exports outputs/predictions.csv
│   ├── utils_text.py
│   └── infer.py
├── data/
│   ├── raw/                           # (ignored) original Kaggle CSV
│   └── processed/                     # (ignored) e.g., reviews_clean.csv
├── models/                            # (ignored) saved .joblib models
├── outputs/
│   ├── predictions.csv                # review-level predictions for BI
│   ├── comparison.csv                 # model metrics summary
│   └── charts/                        # confusion matrices, etc.
├── requirements.txt
├── .gitignore
└── README.md


## Notes & Design Choices

Why SVM + TF-IDF? Strong baseline for short reviews; stable and fast.

Why hybrid sampling? Balances class skew; improved macro-F1 vs plain/oversample.

Reproducibility: Fixed random seeds; clean train/test split; pipeline saved as .joblib.
